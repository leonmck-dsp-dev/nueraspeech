{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/leonmck-dsp-dev/nueraspeech/blob/main/nueraspeech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:57:08.967791Z",
     "start_time": "2024-04-13T20:57:08.965700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:57:09.228427Z",
     "start_time": "2024-04-13T20:57:09.225754Z"
    },
    "id": "LaTtLnEMYUZ-"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:57:09.241053Z",
     "start_time": "2024-04-13T20:57:09.239719Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lalY1_2iYUZ_"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:57:09.256272Z",
     "start_time": "2024-04-13T20:57:09.253823Z"
    },
    "id": "1sjxjWufYUZ_"
   },
   "outputs": [],
   "source": [
    "# Uncomment the line corresponding to your \"runtime type\" to run in Google Colab\n",
    "\n",
    "# CPU:\n",
    "# !pip install pydub torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# GPU:\n",
    "# !pip install pydub torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import sys\n",
    "from torchaudio import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import editdistance\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:57:09.296763Z",
     "start_time": "2024-04-13T20:57:09.294393Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qK_nSa0aYUaA",
    "outputId": "0f619fb2-81e5-4ad7-c89a-c3629ea19211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:59:09.123858Z",
     "start_time": "2024-04-13T20:57:09.316383Z"
    },
    "id": "7EM6WlKQYUaA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.26G/2.26G [01:32<00:00, 26.3MB/s] \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri ./SpeechCommands/speech_commands_v0.02/backward/0165e0e8_nohash_0.wav and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m train_set \u001b[38;5;241m=\u001b[39m SubsetSC(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m test_set \u001b[38;5;241m=\u001b[39m SubsetSC(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m waveform, sample_rate, label, speaker_id, utterance_number \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/nueraspeech/venv/lib/python3.9/site-packages/torchaudio/datasets/speechcommands.py:179\u001b[0m, in \u001b[0;36mSPEECHCOMMANDS.__getitem__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the n-th sample from the dataset.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m        Utterance number\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_metadata(n)\n\u001b[0;32m--> 179\u001b[0m waveform \u001b[38;5;241m=\u001b[39m \u001b[43m_load_waveform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_archive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (waveform,) \u001b[38;5;241m+\u001b[39m metadata[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/Documents/dev/nueraspeech/venv/lib/python3.9/site-packages/torchaudio/datasets/utils.py:51\u001b[0m, in \u001b[0;36m_load_waveform\u001b[0;34m(root, filename, exp_sample_rate)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_waveform\u001b[39m(\n\u001b[1;32m     46\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     47\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     48\u001b[0m     exp_sample_rate: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     49\u001b[0m ):\n\u001b[1;32m     50\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, filename)\n\u001b[0;32m---> 51\u001b[0m     waveform, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exp_sample_rate \u001b[38;5;241m!=\u001b[39m sample_rate:\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample rate should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_sample_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/dev/nueraspeech/venv/lib/python3.9/site-packages/torchaudio/_backend/utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    119\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    120\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[0;32m~/Documents/dev/nueraspeech/venv/lib/python3.9/site-packages/torchaudio/_backend/utils.py:116\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[0;34m(uri, format, backend_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcan_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri ./SpeechCommands/speech_commands_v0.02/backward/0165e0e8_nohash_0.wav and format None."
     ]
    }
   ],
   "source": [
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "import os\n",
    "\n",
    "\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(\"./\", download=True)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as fileobj:\n",
    "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "train_set = SubsetSC(\"training\")\n",
    "test_set = SubsetSC(\"testing\")\n",
    "\n",
    "waveform, sample_rate, label, speaker_id, utterance_number = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:59:09.127757Z",
     "start_time": "2024-04-13T20:59:09.127679Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "0_rUOAj8YUaA",
    "outputId": "d846db2b-05a1-4bf8-e68a-2fe5f0839e08"
   },
   "outputs": [],
   "source": [
    "print(\"Shape of waveform: {}\".format(waveform.size()))\n",
    "print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
    "\n",
    "plt.plot(waveform.t().numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aj6_jO85YUaB",
    "outputId": "85d9f698-f4df-4b96-f21c-9154ece3291b"
   },
   "outputs": [],
   "source": [
    "labels = sorted(list(set(datapoint[2] for datapoint in train_set)))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "0adiSTjFYUaB",
    "outputId": "7cef09ea-3160-4e31-d128-25dfe9e6f4fd"
   },
   "outputs": [],
   "source": [
    "waveform_first, *_ = train_set[0]\n",
    "ipd.Audio(waveform_first.numpy(), rate=sample_rate)\n",
    "\n",
    "waveform_second, *_ = train_set[1]\n",
    "ipd.Audio(waveform_second.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWsWgG_xYUaB"
   },
   "source": [
    "The last file is someone saying “visual”.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "w6AyMcD5YUaB",
    "outputId": "557a5c12-5946-454a-d85e-6e3a69f40ca7"
   },
   "outputs": [],
   "source": [
    "waveform_last, *_ = train_set[-1]\n",
    "ipd.Audio(waveform_last.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "IC172_CtYUaB",
    "outputId": "f60baa77-d75f-41fb-dcaf-7d6ba944f3c8"
   },
   "outputs": [],
   "source": [
    "new_sample_rate = 8000\n",
    "import torchaudio\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class SpecAugment:\n",
    "    def __init__(self, freq_mask_param, time_mask_param, num_time_masks=1, num_freq_masks=1):\n",
    "        self.freq_mask_param = freq_mask_param\n",
    "        self.time_mask_param = time_mask_param\n",
    "        self.num_time_masks = num_time_masks\n",
    "        self.num_freq_masks = num_freq_masks\n",
    "\n",
    "    def __call__(self, specs):\n",
    "        augmented_specs = []\n",
    "        for spec in specs:\n",
    "            # Frequency Masking\n",
    "            for _ in range(self.num_freq_masks):\n",
    "                num_freqs, num_frames = spec.shape\n",
    "                f = random.randrange(0, self.freq_mask_param)\n",
    "                f0 = random.randrange(0, num_freqs - f)\n",
    "                spec[f0:f0 + f, :] = 0\n",
    "\n",
    "            # Time Masking\n",
    "            for _ in range(self.num_time_masks):\n",
    "                num_freqs, num_frames = spec.shape\n",
    "                t = random.randrange(0, self.time_mask_param)\n",
    "                t0 = random.randrange(0, num_frames - t)\n",
    "                spec[:, t0:t0 + t] = 0\n",
    "\n",
    "            augmented_specs.append(spec)\n",
    "\n",
    "        return torch.stack(augmented_specs)\n",
    "\n",
    "class spec_augmentsingle():\n",
    "    def __init__(self, freq_mask_param, time_mask_param, num_time_masks=1, num_freq_masks=1):\n",
    "        self.freq_mask_param = freq_mask_param\n",
    "        self.time_mask_param = time_mask_param\n",
    "        self.num_time_masks = num_time_masks\n",
    "        self.num_freq_masks = num_freq_masks\n",
    "    def __call__(self, spec):\n",
    "    # Frequency Masking\n",
    "        for _ in range(self.num_freq_masks):\n",
    "            num_freqs, num_frames = spec.shape\n",
    "            f = random.randrange(0, self.freq_mask_param)\n",
    "            f0 = random.randrange(0, num_freqs - f)\n",
    "            spec[f0:f0 + f, :] = 0\n",
    "\n",
    "    # Time Masking\n",
    "        for _ in range(self.num_time_masks):\n",
    "            num_freqs, num_frames = spec.shape\n",
    "            t = random.randrange(0, self.time_mask_param)\n",
    "            t0 = random.randrange(0, num_frames - t)\n",
    "            spec[:, t0:t0 + t] = 0\n",
    "    \n",
    "        return spec\n",
    "\n",
    "\n",
    "# 1. Resample\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate)\n",
    "resampled_waveform = resampler(waveform)\n",
    "\n",
    "# 2. Convert to Mel-spectrogram\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=new_sample_rate, n_mels=128)\n",
    "mel_spectrogram = mel_transform(resampled_waveform)\n",
    "\n",
    "# 3. Convert to dB scale\n",
    "db_transform = torchaudio.transforms.AmplitudeToDB()\n",
    "db_mel_spectrogram = db_transform(mel_spectrogram)\n",
    "\n",
    "mel_spectrogram_transform = torch.nn.Sequential(\n",
    "    torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate),\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=new_sample_rate, n_mels=128),\n",
    "    torchaudio.transforms.AmplitudeToDB()\n",
    ")\n",
    "\n",
    "# Instantiate and apply\n",
    "spec_augment = SpecAugment(freq_mask_param=30, time_mask_param=30)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.imshow(db_mel_spectrogram[0].detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Shape of the Mel spectrogram:\", mel_spectrogram.shape)\n",
    "# Listen to the resampled audio\n",
    "ipd.Audio(resampled_waveform.numpy(), rate=new_sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEkZYZirYUaB",
    "outputId": "14abb125-e0ce-47de-dfa0-a28c52a7fddd"
   },
   "outputs": [],
   "source": [
    "def label_to_index(word):\n",
    "    # Return the position of the word in labels\n",
    "    return torch.tensor(labels.index(word))\n",
    "\n",
    "def index_to_label(tensor):\n",
    "    # Convert tensor of indices into its string labels\n",
    "    if tensor.dim() == 0:  # If tensor is a scalar\n",
    "        return labels[tensor.item()]\n",
    "    else:  # If tensor is not a scalar\n",
    "        return [labels[index] for index in tensor.cpu().numpy()]\n",
    "\n",
    "\n",
    "word_start = \"yes\"\n",
    "index = label_to_index(word_start)\n",
    "word_recovered = index_to_label(index)\n",
    "\n",
    "print(word_start, \"-->\", index, \"-->\", word_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10wdIx6tYUaB"
   },
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "    tensors, targets = [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, _, label, *_ in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "# hyperparameters\n",
    "\n",
    "cnn = 3\n",
    "rnn = 5\n",
    "transformers = 4\n",
    "head = 4\n",
    "hidden_dim = 64\n",
    "dropout = 0.1\n",
    "dmodel = 128\n",
    "batch_size = 256\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NsC8Qg3a44vA",
    "outputId": "9b6b80b5-6f86-4a6c-cfc4-ab9935175c7f"
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBuxLZQ_yWlr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcS9MoG6vXhl"
   },
   "source": [
    "# Nueraspeech model architecture\n",
    "## Acoustic Model (Convolutional Layers)\n",
    "\n",
    "### Convolutional Neural Networks (CNNs)\n",
    "A convolutional layer employs a set of learnable filters, which are used to spatially convolve with the input data to produce feature maps. Mathematically, for a 1D input $x$ and a filter $h$, convolution is represented as:\n",
    "\n",
    "$$\n",
    "y(t) = (x * h)(t) = \\sum_{a=-\\infty}^{\\infty} x(a)h(t-a)\n",
    "$$\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "Batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\frac{x - \\text{E}[x]}{\\sqrt{\\text{Var}[x]+\\epsilon}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{x}$ is the normalized data.\n",
    "- $\\text{E}[x]$ is the mean of the batch data.\n",
    "- $\\text{Var}[x]$ is the variance of the batch data.\n",
    "- $\\epsilon$ is a small constant to avoid division by zero.\n",
    "\n",
    "---\n",
    "\n",
    "## Recurrent Layers (GRU Layers)\n",
    "\n",
    "### Gated Recurrent Units (GRUs)\n",
    "\n",
    "GRUs are a type of recurrent neural network (RNN) architecture. The GRU uses gating mechanisms (reset gate and update gate) to control the flow of information.\n",
    "\n",
    "$$\n",
    "r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r)\n",
    "$$\n",
    "$$\n",
    "z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z)\n",
    "$$\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W x_t + r_t \\odot (U h_{t-1} + b) + b')\n",
    "$$\n",
    "$$\n",
    "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $r_t$ is the reset gate.\n",
    "- $z_t$ is the update gate.\n",
    "- $\\tilde{h}_t$ is the candidate activation.\n",
    "- $h_t$ is the output.\n",
    "- $\\sigma$ is the sigmoid function.\n",
    "- $\\odot$ is the element-wise multiplication.\n",
    "\n",
    "---\n",
    "\n",
    "## Transformer Attention\n",
    "\n",
    "### Self Attention\n",
    "\n",
    "The self-attention mechanism allows the model to weigh the significance of each part in a sequence based on its content. For a query $Q$, key $K$, and value $V$ set, the attention mechanism is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "Where $d_k$ is the dimension of the keys.\n",
    "\n",
    "### Multi-head Attention\n",
    "\n",
    "Instead of using one set of attention weights, multi-head attention uses multiple sets, allowing the model to focus on different parts of the input for different tasks or reasons.\n",
    "\n",
    "---\n",
    "\n",
    "## Residual Connections (ResBlocks)\n",
    "\n",
    "Residual connections (or skip connections) allow for the direct flow of gradients during the backpropagation process. If $x$ is the input and $F(x)$ is the output after some layers, the residual connection is:\n",
    "\n",
    "$$\n",
    "\\text{output} = x + F(x)\n",
    "$$\n",
    "\n",
    "This ensures that even as $F(x)$ learns the residuals (or errors), the original input is preserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0peLpt-baoE",
    "outputId": "1b408769-3fa7-427a-a28a-30e34c0154d7"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Variables you might want to change\n",
    "experiment_name = \"experiment2B\"\n",
    "model_name = \"nueraspeech\"\n",
    "layers = 'cnn layers:3 rnn layers:5 transformer layers:4 accuracy: 66% loss: 2.2 avg wer: 0.3 avg cer: 0.2 pre transfer learning'\n",
    "# Paths\n",
    "base_path = os.path.join(\"./results\", experiment_name)\n",
    "plots_path = os.path.join(base_path, \"plots\")\n",
    "models_path = os.path.join(base_path, \"models\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(plots_path, exist_ok=True)\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "plot_file_path_loss = os.path.join(plots_path, \"loss.png\")\n",
    "plot_file_path_accuracy = os.path.join(plots_path, \"accourcy.png\")\n",
    "plot_file_path_pred = os.path.join(plots_path, \"pred.png\")\n",
    "plot_file_path_errorrate = os.path.join(plots_path, \"erro rate.png\")\n",
    "model_file_path = os.path.join(models_path, model_name + \".pth\")\n",
    "\n",
    "plot_file_path_loss, plot_file_path_errorrate, plot_file_path_pred, plot_file_path_accuracy, model_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pW96COf2n9LZ"
   },
   "source": [
    " | experiment  | num cnn | num rnn  | num tranformer | accuarcy |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| 1 | 1 | 1 | 2 | 61% |\n",
    "| 2 | 3| 5 | 4 |  65% |\n",
    "| 3 | 5| 5 | 8 |    |\n",
    "| 4 | 5| 7  | 16 |  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5MfeA7a3WIZ",
    "outputId": "9cfda056-da0c-43df-b652-11ca4a95c600"
   },
   "outputs": [],
   "source": [
    " \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.skip = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        out = out + self.skip(x)\n",
    "        return F.relu(out)\n",
    "class AcousticModel(nn.Module):\n",
    "    def __init__(self, n_input, n_channel, num_res_blocks=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "        # Allow for multiple residual blocks\n",
    "        self.resblocks = nn.Sequential(*[ResBlock(n_channel, n_channel) for _ in range(num_res_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.resblocks(x)\n",
    "        return x\n",
    "\n",
    "class RecurrentLayers(nn.Module):\n",
    "    def __init__(self, n_channel, hidden_dim, num_gru_layers=1):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(n_channel, hidden_dim, num_layers=num_gru_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "        return x\n",
    "    \n",
    "class Transformerblock(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads)  # multi-head attention layer\n",
    "        self.norm1 = nn.LayerNorm(d_model) # layer normalization\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )   # feed-forward network\n",
    "\n",
    "    def forward(self, x):\n",
    "        # For self-attention, query, key, and value are all the same\n",
    "        attended, _ = self.attention(x, x, x)  \n",
    "        x = self.norm1(attended + x) # residual connection and layer normalization\n",
    "        fedforward = self.feed_forward(x) \n",
    "        return self.norm2(fedforward + x) # residual connection and layer normalization\n",
    "\n",
    "class StackedTransformer(nn.Module):\n",
    "    def __init__(self, d_model, heads, N):\n",
    "        super(StackedTransformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([Transformerblock(d_model, heads) for _ in range(N)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class nueraspeechASR(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=35, n_channel=16, hidden_dim=64, dropout_rate=0.4, \n",
    "                 num_res_blocks=cnn, num_gru_layers=rnn, d_model=dmodel, heads=head, num_transformer_layers=transformers):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.acoustic_model = AcousticModel(n_input, n_channel, num_res_blocks=num_res_blocks) \n",
    "        self.dim_matching_conv = nn.Conv1d(n_channel, d_model, kernel_size=1)\n",
    "        self.transformer_to_rnn_fc = nn.Linear(d_model, n_channel)  # n_channel is 16 in this context\n",
    "        self.recurrent_layers = RecurrentLayers(n_channel, hidden_dim, num_gru_layers=num_gru_layers) \n",
    "        self.attention = StackedTransformer(d_model=hidden_dim * 2, heads=heads, N=num_transformer_layers)\n",
    "        self.output_fc = nn.Linear(hidden_dim*2, n_output)\n",
    "    \n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "    def forward(self, x):\n",
    "        x = self.acoustic_model(x)\n",
    "        x = self.dim_matching_conv(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.attention(x)\n",
    "        x = self.transformer_to_rnn_fc(x)\n",
    "        x = self.recurrent_layers(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.output_fc(x) \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Testing the model\n",
    "n_input = 128\n",
    "n_output = 35\n",
    "\n",
    "model = nueraspeechASR(n_input=n_input, n_output=n_output)\n",
    "print(model)\n",
    "\n",
    "# Print number of parameters\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters()):,} trainable parameters')\n",
    "\n",
    "# Test\n",
    "x = torch.randn(1, 128, 8000)\n",
    "OUT = model(x)\n",
    "print(OUT.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiDhhfdA3fbE"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li3m6SMqdY2p"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9zKs9WPdYUaB",
    "outputId": "a323d26d-2146-41e8-b894-2576e9f6bc8b"
   },
   "outputs": [],
   "source": [
    "# @title Default title tex\n",
    "\n",
    "n_epoch = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=0.0001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "pbar = tqdm(total=len(train_loader.dataset), desc=\"Training Progress\")\n",
    "\n",
    "experiment_name = \"experiment1\"\n",
    "model_name = models_path+\"/nueraspeech.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8yIN9wkYUaB"
   },
   "outputs": [],
   "source": [
    "\n",
    "loss_over_epochs = []\n",
    "def train(model, epoch, log_interval, train_loader):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.squeeze(1)  # This will remove the single channel dimension, turning [batch_size, 1, 128, 41] into [batch_size, 128, 41]\n",
    "\n",
    "        # Preprocess the data with Mel-spectrogram transform\n",
    "        data = mel_spectrogram_transform(data)\n",
    "\n",
    "        # Apply SpecAugment (only during training)\n",
    "        data = spec_augment(data)\n",
    "        data = data.to(device)\n",
    "\n",
    "\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "        # record loss\n",
    "    loss_over_epochs.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfXRkUfpEjoe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvhCMAKYYUaC"
   },
   "outputs": [],
   "source": [
    "def wer(s1, s2):\n",
    "\n",
    "    b = set(s1 + s2)\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    w1 = [chr(word2char[w]) for w in s1]\n",
    "    w2 = [chr(word2char[w]) for w in s2]\n",
    "\n",
    "    return editdistance.eval(\" \".join(w1), \" \".join(w2)) / len(s2)\n",
    "def cer(s1, s2): \n",
    "    s1 = \" \".join(s1)\n",
    "    s2 = \" \".join(s2)\n",
    "\n",
    "    return editdistance.eval(s1, s2) / len(s2)\n",
    "\n",
    "\n",
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "wer_over_epochs = []\n",
    "cer_over_epochs = []\n",
    "correct_predictions_over_epochs = []\n",
    "\n",
    "def test(model, epoch, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_wer, total_cer, total_samples = 0, 0, 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        # Apply Mel-spectrogram transform\n",
    "        data = mel_spectrogram_transform(data)\n",
    "\n",
    "        # Remove channel dimension after applying mel transform\n",
    "        data = data.squeeze(1)  # This will convert [batch_size, 1, 128, 41] into [batch_size, 128, 41]\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "\n",
    "        # Compute WER and CER\n",
    "        pred_str = index_to_label(pred)  # Convert the prediction indices to string\n",
    "        target_str = index_to_label(target)  # Convert target indices to string\n",
    "\n",
    "        total_wer += wer(pred_str, target_str)\n",
    "        total_cer += cer(pred_str, target_str)\n",
    "        total_samples += 1\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "\n",
    "    avg_wer = total_wer / total_samples\n",
    "    avg_cer = total_cer / total_samples\n",
    "\n",
    "\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\")\n",
    "    print(f\"Average WER: {avg_wer:.4f}\\nAverage CER: {avg_cer:.4f}\\n\")\n",
    "\n",
    "    return avg_wer, avg_cer, correct, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66TkhMppYUaC",
    "outputId": "613d16ba-b8d7-4d66-a094-eb90bc36e386"
   },
   "outputs": [],
   "source": [
    "log_interval = 20\n",
    "pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
    "losses = []\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "accuracy_over_epochs = []\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    train(model, epoch, log_interval, train_loader)\n",
    "    # Evaluate the model\n",
    "    avg_wer, avg_cer, correct, accuracy = test(model, epoch, test_loader)\n",
    "\n",
    "    # Store the metrics for plotting or further analysis\n",
    "    wer_over_epochs.append(avg_wer)\n",
    "    cer_over_epochs.append(avg_cer)\n",
    "    correct_predictions_over_epochs.append(correct) \n",
    "    scheduler.step()\n",
    "   # Save\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    print(\"saved\")\n",
    "# Let's plot the training loss versus the number of iteration.\n",
    "# plt.plot(losses);\n",
    "# plt.title(\"training loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkN8QHKGy_al"
   },
   "source": [
    "## Metrics\n",
    "\n",
    "### Word Error Rate (WER)\n",
    "\n",
    "Word Error Rate (WER) is a standard metric used to measure the performance of an automatic speech recognition system. It represents the ratio of incorrect words to the total number of words in the reference transcription:\n",
    "\n",
    "$$\n",
    "\\text{WER} = \\frac{\\text{Substitutions + Insertions + Deletions}}{\\text{Total number of words in reference}}\n",
    "$$\n",
    "\n",
    "### Character Error Rate (CER)\n",
    "\n",
    "Character Error Rate (CER) is similar to WER but measures errors at the character level. It represents the ratio of incorrect characters to the total number of characters in the reference transcription:\n",
    "\n",
    "$$\n",
    "\\text{CER} = \\frac{\\text{Substitutions + Insertions + Deletions}}{\\text{Total number of characters in reference}}\n",
    "$$\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Accuracy is a standard metric used to evaluate classification models. It is defined as the ratio of correct predictions to the total number of predictions:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$\n",
    "\n",
    "### Loss\n",
    "\n",
    "The loss function quantifies how well the predicted outputs agree with the actual labels. For classification problems, a common loss function is the negative log-likelihood loss:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\log \\left( \\frac{\\text{Probability of the Correct Class}}{\\text{Sum of Probabilities for All Classes}} \\right)\n",
    "$$\n",
    "\n",
    "These metrics provide a comprehensive evaluation of the model's performance, allowing for better understanding and optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Let's plot the training metrics versus the number of iteration.\n",
    "# WER Plotvimport matplotlib.pyplot as plt\n",
    "epochs = list(range(1, 10))\n",
    "\n",
    "#loss\n",
    "plt.plot(loss_over_epochs, marker='o', label='Training Loss')\n",
    "plt.title('Training Loss Per Batch ' + layers)\n",
    "plt.xlabel(\"Batches\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.title('Correct Predictions Over Epochs '+ layers)\n",
    "\n",
    "\n",
    "\n",
    "#pred\n",
    "plt.plot(correct_predictions_over_epochs, marker='o', label='Number of Correct Predictions')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of Correct Predictions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot WER# Assuming wer_over_epochs and cer_over_epochs have been populated correctly during training:\n",
    "epochs_range = list(range(1, len(wer_over_epochs) + 1))\n",
    "\n",
    "plt.plot(epochs_range, wer_over_epochs, label='WER', marker='o')\n",
    "\n",
    "plt.plot(epochs_range, cer_over_epochs, label='CER', marker='o')\n",
    "plt.title('WER & CER over Epochs ' + layers)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3bm4kkicOFT"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "0M7uLxfoYUaC",
    "outputId": "f3873e24-7e83-4ac3-9a8f-54717b7515a2"
   },
   "outputs": [],
   "source": [
    "def predict(tensor):\n",
    "    # Use the model to predict the label of the waveform\n",
    "    tensor = tensor.to(device)\n",
    "    tensor = transform(tensor)\n",
    "    tensor = model(tensor.unsqueeze(0))\n",
    "    tensor = get_likely_index(tensor)\n",
    "    tensor = index_to_label(tensor.squeeze())\n",
    "    return tensor\n",
    "\n",
    "\n",
    "waveform, sample_rate, utterance, *_ = train_set[-1]\n",
    "ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "\n",
    "print(f\"Expected: {utterance}. Predicted: {predict(waveform)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RtSpxswYUaC"
   },
   "outputs": [],
   "source": [
    "for i, (waveform, sample_rate, utterance, *_) in enumerate(test_set):    output = predict(waveform)\n",
    "if output != utterance:\n",
    "      ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "      print(f\"Data point #{i}. Expected: {utterance}. Predicted: {output}.\")\n",
    "\n",
    "else:\n",
    "    print(\"All examples in this dataset were correctly classified!\")\n",
    "    print(\"In this case, let's just look at the last data point\")\n",
    "    ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "    print(f\"Data point #{i}. Expected: {utterance}. Predicted: {output}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pth = \"results/experiment1/models/nueraspeech.pth\"\n",
    "model.load_state_dict(torch.load(pth))\n",
    "model.eval()\n",
    "print(\"Please provide an audio sample for validation:\")\n",
    "val_waveform = record_audio()\n",
    "val_features = mel_spectrogram_transform(val_waveform)\n",
    "val_features = val_features.squeeze(0).unsqueeze(0)  # Adjusting dimensions\n",
    "predicted_label, wer_score, cer_score = live_validate(model, val_features, )\n",
    "print(f\"Model's prediction: {predicted_label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPODYt4gCwn2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Let's plot the training metrics versus the number of iteration.\n",
    "# WER Plotvimport matplotlib.pyplot as plt\n",
    "epochs = list(range(1, 10))\n",
    "\n",
    "#loss\n",
    "plt.plot(loss_over_epochs, marker='o', label='Training Loss')\n",
    "plt.title('Training Loss Per Batch ' + layers)\n",
    "plt.xlabel(\"Batches\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#pred\n",
    "plt.plot(accuracy_over_epochs, marker='o', label='Number of Correct Predictions')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of Correct Predictions')\n",
    "plt.title('Correct Predictions Over Epochs '+ layers)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot WER# Assuming wer_over_epochs and cer_over_epochs have been populated correctly during training:\n",
    "epochs_range = list(range(1, len(wer_over_epochs) + 1))\n",
    "\n",
    "plt.plot(epochs_range, wer_over_epochs, label='WER', marker='o')\n",
    "\n",
    "plt.plot(epochs_range, cer_over_epochs, label='CER', marker='o')\n",
    "plt.title('WER & CER over Epochs ' + layers)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.labels_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_name = os.path.join(self.root_dir,\n",
    "                                  self.labels_frame.iloc[idx, 2])\n",
    "        waveform, sample_rate = torchaudio.load(audio_name)\n",
    "        label = int(self.labels_frame.iloc[idx, 0]) - 1  # adjust labels from 1-35 to 0-34\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        return waveform, label \n",
    "\n",
    "# Instantiate the dataset\n",
    "audio_dataset = CustomAudioDataset(csv_file='dys/labels.csv',\n",
    "                                   root_dir='dys',\n",
    "                                   transform=None)\n",
    "\n",
    "waveform, label = audio_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(list(set(datapoint[2] for datapoint in train_set)))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Resample\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate)\n",
    "resampled_waveform = resampler(waveform)\n",
    "\n",
    "# 2. Convert to Mel-spectrogram\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=new_sample_rate, n_mels=128)\n",
    "mel_spectrogram = mel_transform(resampled_waveform)\n",
    "\n",
    "# 3. Convert to dB scale\n",
    "db_transform = torchaudio.transforms.AmplitudeToDB()\n",
    "db_mel_spectrogram = db_transform(mel_spectrogram)\n",
    "\n",
    "mel_spectrogram_transform = torch.nn.Sequential(\n",
    "    torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate),\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=new_sample_rate, n_mels=128),\n",
    "    torchaudio.transforms.AmplitudeToDB()\n",
    ")\n",
    "\n",
    "# Instantiate and apply\n",
    "spec_augment = SpecAugment(freq_mask_param=30, time_mask_param=30)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.imshow(db_mel_spectrogram[0].detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Shape of the Mel spectrogram:\", mel_spectrogram.shape)\n",
    "# Listen to the resampled audio\n",
    "ipd.Audio(resampled_waveform.numpy(), rate=new_sample_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_index(word):\n",
    "    # Return the position of the word in labels\n",
    "    return torch.tensor(labels.index(word))\n",
    "\n",
    "def index_to_label(tensor):\n",
    "    # Convert tensor of indices into its string labels\n",
    "    if tensor.dim() == 0:  # If tensor is a scalar\n",
    "        return labels[tensor.item()]\n",
    "    else:  # If tensor is not a scalar\n",
    "        return [labels[index] for index in tensor.cpu().numpy()]\n",
    "\n",
    "\n",
    "word_start = \"yes\"\n",
    "index = label_to_index(word_start)\n",
    "word_recovered = index_to_label(index)\n",
    "\n",
    "print(word_start, \"-->\", index, \"-->\", word_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    tensors, targets = [], []\n",
    "    for waveform, label in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "    return torch.stack(tensors), torch.tensor(targets)\n",
    "\n",
    "train_loaderdys = torch.utils.data.DataLoader(\n",
    "    audio_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.labels_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create a dictionary that maps each label to a unique integer\n",
    "        self.label_mapping = {label: i for i, label in enumerate(sorted(self.labels_frame['Label'].unique()))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "      \n",
    "        audio_name = os.path.join(self.root_dir, self.labels_frame.iloc[idx, 1])\n",
    "        waveform, sample_rate = torchaudio.load(audio_name)\n",
    "\n",
    "        # Convert the label from a string to an integer\n",
    "        label = self.label_mapping[self.labels_frame.iloc[idx, 0]]\n",
    "\n",
    "        # Pad waveform to a fixed length (e.g., 160,000 samples)\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, 160000 - waveform.size(1)))\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        # Remove the channel dimension\n",
    "        waveform = waveform.squeeze(0)\n",
    "\n",
    "        return waveform, label\n",
    "\n",
    "# Instantiate the dataset\n",
    "audio_dataset = CustomAudioDataset(csv_file='dys/labels.csv',\n",
    "                                   root_dir='dys',\n",
    "                                   transform=None)\n",
    "\n",
    "train_loaderdy = DataLoader(audio_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "num_epochs = \n",
    "pth = \"results/experiment1/models/nueraspeech.pth\"\n",
    "model.load_state_dict(torch.load(pth))\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loaderdy):\n",
    "        # Your training code here\n",
    "\n",
    "        data = data.squeeze(1)  # This will remove the single channel dimension, turning [batch_size, 1, 128, 41] into [batch_size, 128, 41]\n",
    "\n",
    "        # Preprocess the data with Mel-spectrogram transform\n",
    "        data = mel_spectrogram_transform(data)\n",
    "\n",
    "        # Apply SpecAugment (only during training)\n",
    "        data = spec_augment(data)\n",
    "        data = data.to(device)\n",
    "\n",
    "\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "        # record loss\n",
    "    loss_over_epochs.append(loss.item())\n",
    "\n",
    "\n",
    "    # Validation loop\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_wer, total_cer, total_samples = 0, 0, 0\n",
    "\n",
    "    for data, target in train_loaderdy:\n",
    "        # Apply Mel-spectrogram transform\n",
    "        data = mel_spectrogram_transform(data)\n",
    "\n",
    "        # Remove channel dimension after applying mel transform\n",
    "        data = data.squeeze(1)  # This will convert [batch_size, 1, 128, 41] into [batch_size, 128, 41]\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "\n",
    "        # Compute WER and CER\n",
    "        pred_str = index_to_label(pred)  # Convert the prediction indices to string\n",
    "        target_str = index_to_label(target)  # Convert target indices to string\n",
    "\n",
    "        total_wer += wer(pred_str, target_str)\n",
    "        total_cer += cer(pred_str, target_str)\n",
    "        total_samples += 1\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "\n",
    "    avg_wer = total_wer / total_samples\n",
    "    avg_cer = total_cer / total_samples\n",
    "\n",
    "\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\")\n",
    "    print(f\"Average WER: {avg_wer:.4f}\\nAverage CER: {avg_cer:.4f}\\n\")\n",
    "    # Save model state\n",
    "    torch.save(model.state_dict(), pth)\n",
    "\n",
    "    wer_over_epochs.append(avg_wer)\n",
    "    cer_over_epochs.append(avg_cer)\n",
    "    correct_predictions_over_epochs.append(correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "# Load the original CSV file\n",
    "df = pd.read_csv('dys/labels.csv')\n",
    "\n",
    "# Define the directory where the audio files are stored\n",
    "root_dir = 'dys'\n",
    "\n",
    "# Generate white noise\n",
    "noise = np.random.normal(0, 1, 44100)\n",
    "for i in range(9):\n",
    "    # Loop over each row in the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        file_path = os.path.join(root_dir, row['Path'])\n",
    "        print(f'Processing {file_path}...')\n",
    "        if os.path.isfile(file_path):\n",
    "            # Load the audio file\n",
    "            audio, _ = sf.read(file_path)\n",
    "\n",
    "            # Generate white noise\n",
    "            noise = np.random.normal(0, 1, len(audio))\n",
    "\n",
    "            # Apply a random transformation to the audio\n",
    "            transformation = random.choice(['noise', 'reverb', 'pitch'])\n",
    "            if transformation == 'noise':\n",
    "                audio = audio + 0.0001 * noise\n",
    "            elif transformation == 'reverb':\n",
    "                audio = audio + np.flip(audio)\n",
    " #           elif transformation == 'pitch':\n",
    "#                audio = np.interp(np.arange(0, len(audio), 0.005), np.arange(len(audio)), audio)\n",
    "\n",
    "            # Generate a random file name\n",
    "            random_file_name = str(uuid.uuid4()) + '.wav'\n",
    "            new_file_path = os.path.join(f'{root_dir}/data', random_file_name)\n",
    "            sf.write(new_file_path, audio, 44100)\n",
    "            print(f'Saved to {new_file_path}')\n",
    "\n",
    "            # Append a new row to the dataframe\n",
    "            df.loc[len(df.index)] = [ row['Label'], 'data/' + random_file_name]\n",
    "\n",
    "        else:\n",
    "            print(f'File not found: {file_path}')\n",
    "\n",
    "# Save the dataframe to the same CSV file\n",
    "df.to_csv('dys/labels.csv', index=False)\n",
    "print('Updated CSV file')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
