{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4900,"status":"ok","timestamp":1686782624714,"user":{"displayName":"Leon Skeptic Official","userId":"09644231496998254674"},"user_tz":-60},"id":"xTflQyoYETk4","outputId":"c0a4f9b7-6a59-49db-a17d-922059b881f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: torchaudio in /opt/homebrew/lib/python3.9/site-packages (0.12.1)\n","Requirement already satisfied: torch in /opt/homebrew/lib/python3.9/site-packages (1.12.1)\n","Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.9/site-packages (from torch) (4.3.0)\n","\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install torchaudio torch"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":214,"status":"ok","timestamp":1686784504554,"user":{"displayName":"Leon Skeptic Official","userId":"09644231496998254674"},"user_tz":-60},"id":"B9GaOo5HFJAX","outputId":"229e2920-d0ee-4d3d-a452-88e0a594adbf"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.9/site-packages/torchaudio/functional/functional.py:539: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n","  warnings.warn(\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as data\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchaudio\n","import numpy as np\n","\n","def avg_wer(wer_scores, combined_ref_len):\n","    return float(sum(wer_scores)) / float(combined_ref_len)\n","def _levenshtein_distance(ref, hyp):\n","    \"\"\"Levenshtein distance is a string metric for measuring the difference\n","    between two sequences. Informally, the levenshtein disctance is defined as\n","    the minimum number of single-character edits (substitutions, insertions or\n","    deletions) required to change one word into the other. We can naturally\n","    extend the edits to word level when calculate levenshtein disctance for\n","    two sentences.\n","    \"\"\"\n","    m = len(ref)\n","    n = len(hyp)\n","\n","    # special case\n","    if ref == hyp:\n","        return 0\n","    if m == 0:\n","        return n\n","    if n == 0:\n","        return m\n","\n","    if m < n:\n","        ref, hyp = hyp, ref\n","        m, n = n, m\n","\n","    # use O(min(m, n)) space\n","    distance = np.zeros((2, n + 1), dtype=np.int32)\n","\n","    # initialize distance matrix\n","    for j in range(0,n + 1):\n","        distance[0][j] = j\n","\n","    # calculate levenshtein distance\n","    for i in range(1, m + 1):\n","        prev_row_idx = (i - 1) % 2\n","        cur_row_idx = i % 2\n","        distance[cur_row_idx][0] = i\n","        for j in range(1, n + 1):\n","            if ref[i - 1] == hyp[j - 1]:\n","                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n","            else:\n","                s_num = distance[prev_row_idx][j - 1] + 1\n","                i_num = distance[cur_row_idx][j - 1] + 1\n","                d_num = distance[prev_row_idx][j] + 1\n","                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n","\n","    return distance[m % 2][n]\n","\n","\n","def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n","    \"\"\"Compute the levenshtein distance between reference sequence and\n","    hypothesis sequence in word-level.\n","    :param reference: The reference sentence.\n","    :type reference: basestring\n","    :param hypothesis: The hypothesis sentence.\n","    :type hypothesis: basestring\n","    :param ignore_case: Whether case-sensitive or not.\n","    :type ignore_case: bool\n","    :param delimiter: Delimiter of input sentences.\n","    :type delimiter: char\n","    :return: Levenshtein distance and word number of reference sentence.\n","    :rtype: list\n","    \"\"\"\n","    if ignore_case == True:\n","        reference = reference.lower()\n","        hypothesis = hypothesis.lower()\n","\n","    ref_words = reference.split(delimiter)\n","    hyp_words = hypothesis.split(delimiter)\n","\n","    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n","    return float(edit_distance), len(ref_words)\n","\n","\n","def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n","    \"\"\"Compute the levenshtein distance between reference sequence and\n","    hypothesis sequence in char-level. \n","    \"\"\"\n","    if ignore_case == True:\n","        reference = reference.lower()\n","        hypothesis = hypothesis.lower()\n","\n","    join_char = ' '\n","    if remove_space == True:\n","        join_char = ''\n","\n","    reference = join_char.join(filter(None, reference.split(' ')))\n","    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n","\n","    edit_distance = _levenshtein_distance(reference, hypothesis)\n","    return float(edit_distance), len(reference)\n","\n","def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n","    \"\"\"Calculate word error rate (WER). WER compares reference text and\n","    hypothesis text in word-level. WER is defined as:\n","    .. math::\n","        WER = (Sw + Dw + Iw) / Nw\n","\n","    \"\"\"\n","    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n","                                         delimiter)\n","\n","    if ref_len == 0:\n","        raise ValueError(\"Reference's word number should be greater than 0.\")\n","\n","    wer = float(edit_distance) / ref_len\n","    return wer\n","\n","\n","def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n","    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n","    hypothesis text in char-level. CER is defined as:\n","    .. math::\n","        CER = (Sc + Dc + Ic) / Nc \n","    \"\"\"\n","    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n","                                         remove_space)\n","\n","    if ref_len == 0:\n","        raise ValueError(\"Length of reference should be greater than 0.\")\n","\n","    cer = float(edit_distance) / ref_len\n","    return cer\n","\n","class TextTransform:\n","    \"\"\"Maps characters to integers and vice versa\"\"\"\n","    def __init__(self):\n","        char_map_str = \"\"\"\n","        ' 0\n","        <SPACE> 1\n","        a 2\n","        b 3\n","        c 4\n","        d 5\n","        e 6\n","        f 7\n","        g 8\n","        h 9\n","        i 10\n","        j 11\n","        k 12\n","        l 13\n","        m 14\n","        n 15\n","        o 16\n","        p 17\n","        q 18\n","        r 19\n","        s 20\n","        t 21\n","        u 22\n","        v 23\n","        w 24\n","        x 25\n","        y 26\n","        z 27\n","        \"\"\"\n","        self.char_map = {}\n","        self.index_map = {}\n","        for line in char_map_str.strip().split('\\n'):\n","            ch, index = line.split()\n","            self.char_map[ch] = int(index)\n","            self.index_map[int(index)] = ch\n","        self.index_map[1] = ' '\n","\n","    def text_to_int(self, text):\n","        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n","        int_sequence = []\n","        for c in text:\n","            if c == ' ':\n","                ch = self.char_map['<SPACE>']\n","            else:\n","                ch = self.char_map[c]\n","            int_sequence.append(ch)\n","        return int_sequence\n","\n","    def int_to_text(self, labels):\n","        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n","        string = []\n","        for i in labels:\n","            string.append(self.index_map[i])\n","        return ''.join(string).replace('<SPACE>', ' ')\n","\n","train_audio_transforms = nn.Sequential(\n","    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n","    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n","    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",")\n","\n","valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n","\n","text_transform = TextTransform()\n","\n","def data_processing(data, data_type=\"train\"):\n","    spectrograms = []\n","    labels = []\n","    input_lengths = []\n","    label_lengths = []\n","    for (waveform, _, utterance, _, _, _) in data:\n","        if data_type == 'train':\n","            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n","        elif data_type == 'valid':\n","            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n","        else:\n","            raise Exception('data_type should be train or valid')\n","        spectrograms.append(spec)\n","        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n","        labels.append(label)\n","        input_lengths.append(spec.shape[0]//2)\n","        label_lengths.append(len(label))\n","\n","    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n","    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n","\n","    return spectrograms, labels, input_lengths, label_lengths\n","\n","\n","def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n","\targ_maxes = torch.argmax(output, dim=2)\n","\tdecodes = []\n","\ttargets = []\n","\tfor i, args in enumerate(arg_maxes):\n","\t\tdecode = []\n","\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n","\t\tfor j, index in enumerate(args):\n","\t\t\tif index != blank_label:\n","\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n","\t\t\t\t\tcontinue\n","\t\t\t\tdecode.append(index.item())\n","\t\tdecodes.append(text_transform.int_to_text(decode))\n","\treturn decodes, targets\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"smCUr3z-F9W5","vscode":{"languageId":"plaintext"}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6aimshHmF88N"},"outputs":[],"source":["class AcousticModel(nn.Module): \n","    def __init__(self):\n","        super(AcousticModel, self).__init__()\n","        \n","        self.conv_layers = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.Hardtanh(0, 20, inplace=True),\n","            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.Hardtanh(0, 20, inplace=True),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.Hardtanh(0, 20, inplace=True),\n","            nn.MaxPool2d(2)\n","        )\n","        \n","        self.lstm = nn.LSTM(64, 128, bidirectional=True, batch_first=True)\n","        self.fc = nn.Linear(256, 29)\n","\n","    def forward(self, x):\n","        x = self.conv_layers(x)\n","        \n","        return x\n","\n","  # Residual cnn layer for extracting and learning audio features\n","  class ResCNN(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel, stride, dropout, nfeatures):\n","        super(ResCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel, stride=stride, padding=kernel//2)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel, stride=stride, padding=kernel//2)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","        self.layer_norm1 = nn.LayerNorm(nfeatures)\n","        self.layer_norm2 = nn.LayerNorm(nfeatures)\n","        \n","    def forward(self, x):\n","        residual = x\n","        x = self.layer_norm1(x)\n","        x = F.relu(x)\n","        x = self.dropout1(x)\n","        x = self.conv1(x)\n","        x = self.layer_norm2(x)\n","        x = F.relu(x)\n","        x = self.dropout2(x)\n","        x = self.conv2(x)\n","        x += residual\n","        return x\n","#  RNN  layer   \n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True) \n","        self.layernorm = nn.LayerNorm(hidden_size*2)\n","        self.fc = nn.Linear(hidden_size*2, num_classes)\n","        self.dropout = nn.Dropout(dropout)\n","    \n","    def forward(self, x):\n","        x = self.layernorm(x)\n","        x = F.relu(x)\n","        x , _ = self.lstm(x)\n","        x = self.dropout(x)\n","        return x\n","#  Fully connected layer for outputting the final prediction\n","class FullyConnected(nn.Module):\n","  def __init__(self, hidden_size, output_size):\n","    super(FullyConnected, self).__init__()\n","    self.fc = nn.Linear(hidden_size*2, output_size)\n","    \n","  def forward(self, x):\n","    x = self.fc(x)\n","    return x\n","# pooling layers\n","class Pooling(nn.Module):\n","  def __init__(self, pool_type):\n","    super(Pooling, self).__init__()\n","    self.pool_type = pool_type\n","    \n","  def forward(self, x):\n","    if self.pool_type == 'max':\n","      x = F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2))\n","    elif self.pool_type == 'avg':\n","      x = F.avg_pool2d(x, kernel_size=(2, 2), stride=(2, 2))\n","    return x\n","        # Reshaping before passing to LSTM batch_size, channels, width, height = x.size() x = x.permute(0, 2, 1, 3).contiguous()  # [batch, width, channels, height] x = x.view(batch_size, width, -1)  # [batch, width, channels*height] x, _ = self.lstm(x) x = self.fc(x) return x class rescnn(nn.Module): def __init__(self): super(rescnn, self).__init__() self.conv_layers = nn.Sequential( nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(32), nn.ReLU(),  nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(64), nn.MaxPool2d(2)) def forward(self, x): x = self.conv_layers(x) return x\n","class GRU(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):\n","        super(GRU, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True) \n","        self.layernorm = nn.LayerNorm(hidden_size*2)\n","        self.fc = nn.Linear(hidden_size*2, num_classes)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.layernorm(x)\n","        x = F.relu(x)\n","        x , _ = self.gru(x)\n","        x = self.dropout(x)\n","        return x\n","class Nueraspeechmodel(nn.Module):\n","    def __init__(self):\n","        super(Nueraspeechmodel, self).__init__()\n","        self.acoustic_model = AcousticModel() \n","        self.rescnn = ResCNN(in_channels=64, out_channels=64, kernel=3, stride=1, dropout=0.1, nfeatures=64)\n","        self.rnn = RNN(input_size=64*8*8 , hidden_size=128, num_layers=1, num_classes=256, dropout=0.1)\n","        self.lstm1 = nn.LSTM(256, 128, bidirectional=True, batch_first=True)\n","        self.lstm2 = nn.LSTM(256, 128, bidirectional=True, batch_first=True)\n","        self.lstm3 = nn.LSTM(256, 128, bidirectional=True, batch_first=True)\n","        self.lstm4 = nn.LSTM(256, 128, bidirectional=True, batch_first=True)\n","        self.fc1 = FullyConnected(hidden_size=256, output_size=29)   # 29 for 26 alphabets + space, apostrophe, and blank \n","\n","    def forward(self, x):\n","        x = self.acoustic_model(x)\n","        x = self.rescnn(x)\n","        x = x.view(x.size(0), x.size(1), -1)  # Reshape before passing to RNN\n","        x = self.rnn(x)\n","        x, _ = self.lstm1(x)\n","        x, _ = self.lstm2(x)\n","        x, _ = self.lstm3(x)\n","        x, _ = self.lstm4(x)\n","        x = self.fc1(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# training code for the neural network  \n","\n","class Iterator(object):\n","    def __init__(self, data, batch_size):\n","        self.data = data\n","        self.batch_size = batch_size\n","        self.idx = 0\n","        self.max_idx = len(data)\n","        self.epoch = 0\n","        self.is_new_epoch = False\n","        self.reset()\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        if self.is_new_epoch:\n","            self.is_new_epoch = False\n","            raise StopIteration\n","        else:\n","            return self.next()\n","\n","    def next(self):\n","        if self.idx >= self.max_idx:\n","            self.is_new_epoch = True\n","            self.reset()\n","            raise StopIteration\n","\n","        i = self.idx\n","        j = min(i + self.batch_size, self.max_idx)\n","        self.idx = j\n","        return self.data[i:j]\n","\n","    def reset(self):\n","        self.idx = 0\n","        self.epoch += 1\n","        np.random.shuffle(self.data)\n","\n","\n","def train(model,device,train_loader,optimizer,epoch,criterion, scheduler, iteration, exoeriment): \n","    model.train() \n","    data_len = len(train_loader.dataset)\n","    with exoeriment.train():\n","        for batch_idx, _data in enumerate(train_loader):\n","            spectrograms, labels, input_lengths, label_lengths = _data \n","            spectrograms, labels = spectrograms.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            output = model(spectrograms)\n","            output = F.hardtanh(output,0,20)\n","            output = output.transpose(0, 1) # (time, batch, n_class)]\n","\n","\n","\n","\n","\n","\n","def test(model, device, test_loader, criterion, epoch, iter_meter, experiment):\n","    print('\\nevaluating…')\n","    model.eval()\n","    test_loss = 0\n","    test_cer, test_wer = [], []\n","    with experiment.test():\n","        with torch.no_grad():\n","            for I, _data in enumerate(test_loader):\n","                spectrograms, labels, input_lengths, label_lengths = _data \n","                spectrograms, labels = spectrograms.to(device), labels.to(device)\n","\n","                output = model(spectrograms)  # (batch, time, n_class)\n","                output = F.log_softmax(output, dim=2)\n","                output = output.transpose(0, 1) # (time, batch, n_class)\n","\n","                loss = criterion(output, labels, input_lengths, label_lengths)\n","                test_loss += loss.item() / len(test_loader)\n","\n","                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n","                for j in range(len(decoded_preds)):\n","                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n","                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n","\n","\n","    avg_cer = sum(test_cer)/len(test_cer)\n","    avg_wer = sum(test_wer)/len(test_wer)\n","    experiment.log_metric('test_loss', test_loss, step=iter_meter.get())\n","    experiment.log_metric('cer', avg_cer, step=iter_meter.get())\n","    experiment.log_metric('wer', avg_wer, step=iter_meter.get())\n","\n","    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n","\n","def main(learning_rate=5e-4, batch_size=20, epochs=10,\n","        train_url=\"train-clean-100\", test_url=\"test-clean\",\n","        experiment=Experiment(api_key='dummy_key', disabled=True)):\n","\n","    hparams = {\n","        \"n_cnn_layers\": 3,\n","        \"n_rnn_layers\": 5,\n","        \"rnn_dim\": 512,\n","        \"n_class\": 29,\n","        \"n_feats\": 128,\n","        \"stride\": 2,\n","        \"dropout\": 0.1,\n","        \"learning_rate\": learning_rate,\n","        \"batch_size\": batch_size,\n","        \"epochs\": epochs\n","    }\n","\n","    experiment.log_parameters(hparams)\n","\n","    use_cuda = torch.cuda.is_available()\n","    torch.manual_seed(7)\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if not os.path.isdir(\"./data\"):\n","        os.makedirs(\"./data\")\n","\n","    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n","    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n","\n","    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","    train_loader = data.DataLoader(dataset=train_dataset,\n","                                batch_size=hparams['batch_size'],\n","                                shuffle=True,\n","                                collate_fn=lambda x: data_processing(x, 'train'),\n","                                **kwargs)\n","    test_loader = data.DataLoader(dataset=test_dataset,\n","                                batch_size=hparams['batch_size'],\n","                                shuffle=False,\n","                                collate_fn=lambda x: data_processing(x, 'valid'),\n","                                **kwargs)\n","\n","    model = Nueraspeechmodel(\n","        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n","        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n","        ).to(device)\n","\n","    print(model)\n","    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n","\n","    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n","    criterion = nn.CTCLoss(blank=28).to(device)\n","    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n","                                            steps_per_epoch=int(len(train_loader)),\n","                                            epochs=hparams['epochs'],\n","                                            anneal_strategy='linear')\n","\n","    iter_meter = Iterator()\n","    for epoch in range(1, epochs + 1):\n","        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment)\n","        test(model, device, test_loader, criterion, epoch, iter_meter, experiment)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPmmD3lspBMo5rT/t5tC2+F","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
